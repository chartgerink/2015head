Megan Head and colleagues provide a large collection of p-values that, from their perspective, indicates widespread statistical significance seeking (i.e., p-hacking) throughout the sciences. The analyses that form the basis of their conclusions operate on the tenet that p-hacked papers show p-value distributions that are left skew under .05 \cite{Simonsohn2014}. In this paper I evaluate their data analytic choices and strategy and show that these alter their results substantially. 

I applaud their transparency in sharing both the data and analysis scripts, allowing for thorough assessment of the data analytic process and allowing alternative data analytic perspectives. In line with their original openness, I version controlled all my research efforts, allowing the assessment of my own data analytic process. Version control provides a timestamped history of the changes made to files, such as the analysis code or writing \cite{Ram2013}. This is comparable to track changes, but applied to computer files. The version control of this paper is available at [https://osf.io/sxafg/](https://osf.io/sxafg/).

This paper is structured into four parts: (i) evaluation of data analytic choices made by Head and colleagues, (ii) reevaluating the results based on my alternative choices but the same data analytic approach (i.e., sensitivity reanalysis), (iii) evaluating the data analytic strategy, and (iv) reevaluating the results of a different data analytic strategy (i.e., strong reanalysis). 