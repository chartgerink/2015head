In their original analyses, Head and colleagues use four selection steps that require some justification. These four steps encompass selecting only (i) papers with one Digital Object Identifier (DOI), (ii) papers with non-zero authors, (iii) p-values smaller than .05 (i.e., $<.05$), and (iv) exactly reported p-values (i.e., $p=...$). Below, I evaluate these four non-standard choices, which could affect results. I argue that (i) and (iii) seem invalid and (ii) and (iv) seem valid.

Choice (i) results in retaining only those papers with one DOI, which seems conservative. Retaining only papers with exactly one DOI results in the elimination of $84409$ p-values from XX papers. However, no substantive reason is given to eliminate the papers without a DOI or with multiple DOIs and I cannot argue t. In fact, eliminating those without DOIs would result in eliminating p-values from perfectly valid articles or older articles, considering DOIs were only initiated in 1999 \cite{Crossre2009}. Upon inspecting some of the articles with multiple DOIs, these additional DOIs proved to be advanced online publications of the paper under a different DOI that were linked to or discussion papers that accompany the original paper. Retaining the papers with zero or $>1$ DOIs therefore seems warranted.

Choice (ii) includes the removal of zero-author papers, which is justifiable. Zero-author papers are most likely editorials, corrections, retractions, etc. These "papers" provide only little information and the p-values they do report are not original p-values but reproductions provided in another paper, most likely. Eliminating these therefore seems a wise choice.

Choice (iii) selects all p-values $<.05$, because the original authors "suspect that many authors do not regard $p=.05$ as significant" \cite{Head2015}. Previous investigation of p-values reported as exactly .05 revealed that $94.3\%$ of 236 cases interpret this as statistically significant \cite{Nuijten2015}. This goes against Head and colleagues their assumption that most researchers do not interpret $p=.05$ as significant, which is why I argue that the selection should be $p\leq.05$ and not $p<.05$.

Choice (iv) retains only the exactly reported p-values, which eliminates potential bias due to significance thresholds \cite{Ridley2007}. Additionally, if frequencies of inexactly reported results (e.g., $p<.05$) are included in the analysis, this artefactually inflates the frequency of those significance thresholds, which would increase the "evidence" for p-hacking. 