\section*{Sensitivity reanalysis}
The previous section indicated that two of four data analytic choices are disputable, which warrants sensitivity reanalysis. I define a sensitivity reanalysis as a reanalysis where only adjustments are made to the data selection and it is inspected whether original results hold. This is opposite to a full reanalysis, where changes are made to the data analytic strategy. In this sensitivity reanalysis I eliminated the selection based on DOIs and changed the selection of $p<.05$ into $p\leq.05$.

As a result of these changes in the reanalysis, evidence for p-hacking became stronger across the board, mainly due to changing the selection of $p<.05$ to $p\leq.05$. For example, the evidence for p-hacking in the Results section across all fields was strong originally ($P=.546$ where $P$ is proportion, lower $95\%$ CI $[.535],p<.001$) and even stronger afterwards ($P=.731$, lower $95\%$ CI $[.723],p<.001$). All other results also showed stronger evidence for p-hacking in the reanalysis than in the original results. However, in the next section I argue that this data analytic strategy is suboptimal and that a different strategy removes all evidence for p-hacking.
  