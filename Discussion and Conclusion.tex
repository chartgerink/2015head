\section*{Discussion} 
The current reanalysis finds no evidence for widespread left-skew p-hacking. This might seem inconsistent with previous findings, such as the low replication rates in psychology \cite{Baker_undated-qd} or self-reports of p-hacking \cite{John2012-uj}. However, these results are not necessarily inconsistent because they are not mutually exclusive. 

Low replication rates could be caused by widespread p-hacking, but can also occur under systemic low power \cite{Bakker2014-lr,Bakker_2012}. Previous research has indicated low power levels in, for example, psychology \cite{Cohen1962-jc,Sedlmeier1989-yc} and randomized clinical trials \cite{Moher1994-ra}. As a consequence of low power some argue that there is a high prevalence of false positives \cite{Ioannidis2005-am}, which would result in low replication rates.

Additionally, high admission rates of p-hacking found by \cite{John2012-uj} pertain to admission of such behaviors occurring at least once. Even if there is widespread occurrence across researchers, this does not necessitate frequent occurrence. A researcher might admit to having p-hacked sometime during his career, but this does not necessitate that it occurred frequently. Moreover, as noted in the introduction, not all p-hacking behaviors lead to left-skew in the p-value distribution. The method used to detect p-hacking in this paper is sensitive to only left-skew p-hacking and does not provide evidence for absence of p-hacking.

In this reanalysis two minor limitations remain with respect to the data analysis. First, selecting the bins just below .04 and .05 results in selecting non-adjacent bins. Hence, the test might be less sensitive to detecting left-skew p-hacking. In light of this limitation I ran the original analysis but included the second decimal, which resulted in the comparison of $.04\leq p<.045$ versus $.045<p\leq.05$. This analysis also yielded no evidence for left-skew p-hacking, $Prop.=.457,p>.999,BF_{10}<.001$. Second, the selection of only exactly reported p-values might have distorted the p-value distribution due to minor rounding biases. Previous research has indicated that p-values are somewhat more likely to be rounded to .05 rather than to .04 \cite{Krawczyk2015-uh}. Therefore, selecting only exactly reported p-values might cause an underrepresentation of .05 values, because p-values are more frequently rounded and reported as $<.05$ instead of exactly (e.g., $p=.046$). This limitation also applies to the original paper by Head et al. and is therefore a general limitation to analyzing p-value distributions.

\section*{Conclusion}
The conclusion that there is a lack of evidence for left-skew p-hacking remains in this reanalysis, even in light of the limitations of the current reanalysis. 
These reanalyses indicate that the evidence for p-hacking is either underestimated (sensitivity reanalysis) or artefactual  (full reanalysis). The problem of second-decimal reporting bias is, in my opinion, something that cannot be neglected and therefore I conclude that the data provided by Head and colleagues yields no substantial evidence for widespread p-hacking throughout the sciences. This is in line with a previous reanalysis that indicated that the paper that initiated the line of research into p-hacking \cite{Masicampo2012} did not provide sufficient evidence for p-hacking after \cite{Lakens2014}. In sum, this reanalysis yields no evidence for widespread p-hacking.
  
  
  